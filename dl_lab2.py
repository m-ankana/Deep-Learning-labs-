# -*- coding: utf-8 -*-
"""DL-Lab2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HJTnWVlorPhyiZeo0P0Hp2mwukNvLeSf
"""

# Execute this code block to install dependencies when running on colab
try:
    import torch
except:
    from os.path import exists
    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision

!pip install scikit-learn==0.20.4

from typing import Tuple

def gd_factorise_ad(A: torch.Tensor, rank: int, num_epochs=1000,lr=0.01)-> Tuple[torch.tensor, torch.Tensor]:
  m,n = A.shape
  if(rank< min(m,n)):
    U=torch.rand((m,rank),requires_grad=True, dtype=torch.float)
    V=torch.rand((n,rank), requires_grad=True, dtype=torch.float)
  for epoch in range(num_epochs):
    # manually dispose of the gradient )
    U.grad = None
    V.grad = None
    
    # prediction
    Y_pred = U @ torch.t(V)

    # loss function
    loss = torch.nn.functional.mse_loss(Y_pred, A,reduction='sum') 
    loss.backward()
    #compute update
    U.data -= lr*U.grad
    V.data -= lr*V.grad
  return U, V

A_list=[[0.3374,0.6005,0.1735],[3.3359,0.0492,1.8374],[2.9407,0.5301,2.2620]]
A= torch.Tensor(A_list)

u,v=gd_factorise_ad(A,2)
torch.nn.functional.mse_loss(u @ torch.t(v), A, reduction='sum')

import pandas as pd
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'+'/iris/iris.data', header=None)
data=torch.tensor(df.iloc[:, [0,1,2,3]].values , dtype=torch.float)
data = data - data.mean(dim=0)
u,v = gd_factorise_ad(data,2)
# data_hat= torch.mm(u,torch.t(v))
# reconstruction_loss(data,data_hat)
torch.nn.functional.mse_loss(u @ torch.t(v), data, reduction='sum')

U, s, Vh = torch.svd(data)
U_transpose = torch.t(U)
S_diag = torch.diag(s)
data_hat_svd = U@S_diag@(torch.t(Vh))
print("MSE for svd : ", torch.nn.functional.mse_loss(data_hat_svd, data, reduction='sum'))

"""1.3 Compare against PCA !"""



"""2. A simple MLP"""

import pandas as pd 
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'+'/iris/iris.data', header=None)
df = df.sample(frac = 1) 

# add label indices column
mapping = {k : v for v, k in enumerate(df[4].unique())}
df[5] = df[4].map(mapping)

# normalise data
alldata = torch.tensor(df.iloc[:, [0,1,2,3]].values, dtype = torch.float)
alldata = ( alldata - alldata.mean( dim=0)) / alldata.var(dim=0)

# create datasets
targets_tr = torch.tensor( df.iloc[:100, 5].values, dtype = torch.long)
targets_va = torch.tensor( df.iloc[100:, 5].values, dtype = torch.long)
data_tr = alldata[ : 100]
data_va = alldata[100 : ]

def mlp(A: torch.Tensor, num_epochs=100,lr=0.01) -> Tuple[torch.tensor, torch.Tensor, torch.Tensor , torch.Tensor]:
  # initialisation
  W1 = torch.normal(2,3, size=(4,12), requires_grad=True)
  W2 = torch.normal(2,3, size=(12,4) , requires_grad=True)
  b1 = torch.tensor([0], requires_grad=True , dtype=torch.float)
  b2 = torch.tensor([0], requires_grad=True , dtype=torch.float)
  for epoch in range(num_epochs):
    W1.grad = None
    W2.grad = None
    b1.grad = None
    b2.grad = None
    # prediction
    logits = torch.relu(A@W1 +b1)@ W2 + b2
    # print(logits.shape, A.shape)
    # loss function 
    loss = torch.nn.functional.cross_entropy(logits , targets_tr)
    #auto-compute the gradients 
    loss.backward() 
    #compute update
    W1.data -= lr*W1.grad
    W2.data -= lr*W2.grad
    b1.data -= lr*b1.grad
    b2.data -= lr*b2.grad
  return W1,W2,b1,b2

W1 , W2, b1, b2 = mlp(data_tr)
y_pred = torch.relu(data_va@W1 +b1)@ W2 + b2
print(torch.nn.functional.cross_entropy(y_pred , targets_va))