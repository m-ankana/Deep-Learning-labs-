# -*- coding: utf-8 -*-
"""DL-Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1beDfco56OFGthm26v4diL-ZoMIdZbxmt
"""

# Execute this code block to install dependencies when running on colab
try:
    import torch
except:
    from os.path import exists
    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision

!pip install scikit-learn==0.20.4

"""1. Implement a matrix factorisation using gradient descent """

from typing import Tuple

def sgd_factorise(A: torch.Tensor, rank: int, num_epochs=1000, lr=0.01)-> Tuple[torch.Tensor, torch.tensor]:
  m,n = A.shape
  if(rank< min(m,n)):
    
    U= torch.normal(0, 1, size=(m,rank))
    V= torch.normal(0, 1, size=(n,rank))
  for epoch in range(num_epochs):
    for r in range(m):
      for c in range(n):
        e = A[r][c]-U[r]@torch.t(V[c])
        U[r] = U[r] + lr*e*V[c]
        V[c] = V[c] + lr*e*U[r]
  return U, V

A_list=[[0.3374,0.6005,0.1735],[3.3359,0.0492,1.8374],[2.9407,0.5301,2.2620]]
A= torch.Tensor(A_list)

U,V = sgd_factorise(A,2)

A_hat_sgd= torch.mm(U,torch.t(V))

print("MSE for sgd: ", torch.nn.functional.mse_loss( A ,A_hat_sgd ,reduction='sum'))

"""2. Compare result to truncated SVD """

U, S, Vh = torch.svd(A)
print(S)
S[2] = 0
U_transpose = torch.t(U)
S_diag = torch.diag(S)
A_hat = U@S_diag@(torch.t(Vh))

print("MSE for svd : ", torch.nn.functional.mse_loss(A_hat, A, reduction='sum'))

"""3. Matrix completion"""

def sgd_factorise_masked(A: torch.Tensor, M:torch.Tensor, rank: int, num_epochs=1000, lr=0.01)-> Tuple[torch.Tensor, torch.tensor]:
  m,n = A.shape
  if(rank< min(m,n)):
    U= torch.normal(0, 1, size=(m,rank))
    V= torch.normal(0, 1, size=(n,rank))
  for epoch in range(num_epochs):
    for r in range(m):
      for c in range(n):
        if M[r][c] == 1:
          e = A[r][c]-U[r]@torch.t(V[c])
          U[r] = U[r] + lr*e*V[c]
          V[c] = V[c] + lr*e*U[r]

  return U, V

# binary masking function 
def matrix_binary_mask(input):
  m, n = input.shape
  M = torch.empty((m,n), dtype=torch.int64)
  for r in range(m):
      for c in range(n):
        if torch.isnan(input[r][c]) :
          M[r][c]= 0
        else:
          M[r][c]=1
  return M

B_list=[[0.3374,0.6005,0.1735],[float('nan'),0.0492,1.8374],[2.9407,float('nan'),2.2620]]
B = torch.tensor(B_list , dtype= torch.float )
# print(B)
print(matrix_binary_mask(B))

U_,V_=sgd_factorise_masked(B,matrix_binary_mask(B),2)

B_= torch.mm(U_,torch.t(V_))
print(B_)

print("MSE for svd : ", torch.nn.functional.mse_loss(A, B_, reduction='sum'))